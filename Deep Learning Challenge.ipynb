{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "254d0107-5a82-4fa3-afc6-ada0db8bce08",
   "metadata": {},
   "source": [
    "### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2739b80-d610-4567-a892-372e2a1a6bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import datetime\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d95a7f5-d50e-4212-a181-8b3bb8a767b3",
   "metadata": {},
   "source": [
    "### Load train images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a94ada16-d007-479d-82ea-00086713bb96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Image_1.jpg</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Image_2.jpg</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Image_3.jpg</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Image_4.jpg</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Image_5.jpg</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      filename   label\n",
       "0  Image_1.jpg    male\n",
       "1  Image_2.jpg  female\n",
       "2  Image_3.jpg  female\n",
       "3  Image_4.jpg  female\n",
       "4  Image_5.jpg    male"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv(\"eye_gender_data/Training_set.csv\") # loading the labels\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbbe9e8b-3be9-4449-bbb3-ee4721c99658",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [[fname, 'eye_gender_data/train/' + fname] for fname in labels['filename']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f484c7df-eaf7-4439-9859-a7d6b1f2d2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels i.e.  9220 matches the number of filenames i.e.  9220\n"
     ]
    }
   ],
   "source": [
    "# Confirm if number of images is same as number of labels given\n",
    "if len(labels) == len(file_paths):\n",
    "    print('Number of labels i.e. ', len(labels), 'matches the number of filenames i.e. ', len(file_paths))\n",
    "else:\n",
    "    print('Number of labels does not match the number of filenames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4ffdb89-02fc-452b-ae7f-f4e9327f546d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>filepaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Image_1.jpg</td>\n",
       "      <td>eye_gender_data/train/Image_1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Image_2.jpg</td>\n",
       "      <td>eye_gender_data/train/Image_2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Image_3.jpg</td>\n",
       "      <td>eye_gender_data/train/Image_3.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Image_4.jpg</td>\n",
       "      <td>eye_gender_data/train/Image_4.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Image_5.jpg</td>\n",
       "      <td>eye_gender_data/train/Image_5.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      filename                          filepaths\n",
       "0  Image_1.jpg  eye_gender_data/train/Image_1.jpg\n",
       "1  Image_2.jpg  eye_gender_data/train/Image_2.jpg\n",
       "2  Image_3.jpg  eye_gender_data/train/Image_3.jpg\n",
       "3  Image_4.jpg  eye_gender_data/train/Image_4.jpg\n",
       "4  Image_5.jpg  eye_gender_data/train/Image_5.jpg"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = pd.DataFrame(file_paths, columns=['filename', 'filepaths'])\n",
    "images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c9faf93-2524-4ff9-89be-f0cbb40cdb0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>filepaths</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Image_1.jpg</td>\n",
       "      <td>eye_gender_data/train/Image_1.jpg</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Image_2.jpg</td>\n",
       "      <td>eye_gender_data/train/Image_2.jpg</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Image_3.jpg</td>\n",
       "      <td>eye_gender_data/train/Image_3.jpg</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Image_4.jpg</td>\n",
       "      <td>eye_gender_data/train/Image_4.jpg</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Image_5.jpg</td>\n",
       "      <td>eye_gender_data/train/Image_5.jpg</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      filename                          filepaths   label\n",
       "0  Image_1.jpg  eye_gender_data/train/Image_1.jpg    male\n",
       "1  Image_2.jpg  eye_gender_data/train/Image_2.jpg  female\n",
       "2  Image_3.jpg  eye_gender_data/train/Image_3.jpg  female\n",
       "3  Image_4.jpg  eye_gender_data/train/Image_4.jpg  female\n",
       "4  Image_5.jpg  eye_gender_data/train/Image_5.jpg    male"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.merge(images, labels, how = 'inner', on = 'filename')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2747b9ad-fa68-4e6d-aad2-9bcbafe37863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "train_data['label'] = le.fit_transform(train_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e4f18a6-5481-4c4c-9eb9-0f463f404b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [] # initialize an empty numpy array\n",
    "image_size = 53\n",
    "for i in range(len(train_data)):\n",
    "\n",
    "    img_array = cv2.imread(train_data['filepaths'][i], cv2.IMREAD_GRAYSCALE) # converting the image to gray scale\n",
    "    new_img_array = cv2.resize(img_array, (image_size, image_size)) # resizing the image array\n",
    "    data.append([new_img_array, train_data['label'][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "375ad410-d980-46bf-8ac8-67e00668fe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for image in data:\n",
    "    x.append(image[0])\n",
    "    y.append(image[1])\n",
    "\n",
    "# converting x & y to numpy array as they are list\n",
    "train_images = np.array(x)\n",
    "train_labels = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8913b5b2-1300-46da-9a09-ed48bac69aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([4162, 5058], dtype=int64))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b38a8ec-8c8d-41e8-860a-4948cdc5369b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 53)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of an image\n",
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "232d5dd7-8c62-4b48-9d0d-f2c9d75e5c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9220, 53, 53)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2afe6ba3-75a3-44d2-81f9-e6b6d5287719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train_images.shape: (9220, 53, 53, 1), of uint8\n"
     ]
    }
   ],
   "source": [
    "# reshape for feeding into the model\n",
    "train_images_gr = train_images.reshape(train_images.shape[0], 53, 53, 1)\n",
    "\n",
    "print('\\nTrain_images.shape: {}, of {}'.format(train_images_gr.shape, train_images_gr.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7423a38-470f-404a-bd06-417a37684410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize values\n",
    "train_images = train_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f64710-c6c5-450c-b314-c2f9bd2ad524",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2915269-defc-4cf6-b13f-819725d291c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential([\n",
    "tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3),  strides=(1, 1), activation='relu', padding='valid', input_shape=(53, 53, 1)),\n",
    "tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3),  strides=(1, 1), activation='relu', padding='valid', input_shape=(53, 53, 1)),\n",
    "tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "tf.keras.layers.Flatten(),\n",
    "    \n",
    "tf.keras.layers.Dense(128, activation='relu'),\n",
    "tf.keras.layers.Dropout(rate=0.3),\n",
    "    \n",
    "tf.keras.layers.Dense(2, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cce8bfaa-f4d8-4204-9889-0900b0cff70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 51, 51, 16)        160       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 25, 25, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 23, 23, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 11, 11, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3872)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               495744    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 500,802\n",
      "Trainable params: 500,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.compile(optimizer='adam',\n",
    "loss='sparse_categorical_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "\n",
    "earlystopping_callback = EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=2, \n",
    "    verbose=1, restore_best_weights=True, mode='auto'\n",
    ")\n",
    "\n",
    "# view model layers\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0398207-ba1a-4ba2-bc78-84876e0f5df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "116/116 [==============================] - 11s 94ms/step - loss: 0.5895 - accuracy: 0.6870 - val_loss: 0.4857 - val_accuracy: 0.7782\n",
      "Epoch 2/50\n",
      "116/116 [==============================] - 11s 92ms/step - loss: 0.4345 - accuracy: 0.8053 - val_loss: 0.3668 - val_accuracy: 0.8476\n",
      "Epoch 3/50\n",
      "116/116 [==============================] - 11s 98ms/step - loss: 0.3738 - accuracy: 0.8416 - val_loss: 0.3267 - val_accuracy: 0.8666\n",
      "Epoch 4/50\n",
      "116/116 [==============================] - 11s 97ms/step - loss: 0.3328 - accuracy: 0.8625 - val_loss: 0.3087 - val_accuracy: 0.8747\n",
      "Epoch 5/50\n",
      "116/116 [==============================] - 11s 98ms/step - loss: 0.3153 - accuracy: 0.8715 - val_loss: 0.2847 - val_accuracy: 0.8883\n",
      "Epoch 6/50\n",
      "116/116 [==============================] - 11s 96ms/step - loss: 0.2848 - accuracy: 0.8826 - val_loss: 0.2742 - val_accuracy: 0.8883\n",
      "Epoch 7/50\n",
      "116/116 [==============================] - 11s 94ms/step - loss: 0.2689 - accuracy: 0.8879 - val_loss: 0.2851 - val_accuracy: 0.8802\n",
      "Epoch 8/50\n",
      "104/116 [=========================>....] - ETA: 1s - loss: 0.2517 - accuracy: 0.8998"
     ]
    }
   ],
   "source": [
    "history = cnn.fit(x=train_images, y=train_labels, epochs=50, batch_size=64, validation_split=0.2, callbacks=[earlystopping_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a743e095-30e3-4519-9797-bc25eed90394",
   "metadata": {},
   "source": [
    "### Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268e6d42-295e-4ad6-b39f-06cfdf84a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(22, 8))\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df[['loss', 'val_loss']].plot(kind='line', ax=ax[0])\n",
    "history_df[['accuracy', 'val_accuracy']].plot(kind='line', ax=ax[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a16d6af-c603-4f60-94b3-9290efdfa665",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a133fdd4-dcc6-49b8-bc84-a925497b5f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = pd.read_csv(\"eye_gender_data/Testing_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755bd988-e464-40e6-b6f2-7e19373f4f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [[fname, 'eye_gender_data/test/' + fname] for fname in test_labels['filename']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738a3f94-fd8e-4566-a16f-f269438f9a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm if number of images is same as number of labels given\n",
    "if len(test_labels) == len(file_paths):\n",
    "    print('Number of labels i.e. ', len(test_labels), 'matches the number of filenames i.e. ', len(file_paths))\n",
    "else:\n",
    "    print('Number of labels does not match the number of filenames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2832531e-c733-47b0-9462-a5c0587eac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame(file_paths, columns=['filename', 'filepaths'])\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1416a5-678b-43db-9af3-797b2a9b3016",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "image_size = 53\n",
    "for i in range(len(test_data)):\n",
    "\n",
    "    img_array = cv2.imread(test_data['filepaths'][i], cv2.IMREAD_GRAYSCALE) # converting the image to gray scale\n",
    "    new_img_array = cv2.resize(img_array, (image_size, image_size)) # resizing the image array\n",
    "    data.append([new_img_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20903d7d-130c-46e0-be95-b6514b14cf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for image in data:\n",
    "    x.append(image[0])\n",
    "\n",
    "# converting x & y to numpy array as they are lists\n",
    "test_images = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c448d710-c93f-4c93-88f8-3986677df4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_gr = test_images.reshape(test_images.shape[0], 53, 53, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3204cf2b-aafe-42fa-b1ef-c62a8aaa635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea691c1c-f965-4373-b0e1-000020faefe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cnn.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a813116-de98-4120-a0f1-dce05f5c252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_labels = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaaa9a7-cdb4-4771-81b3-4b6643fb5ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'label': prediction_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5663f928-0c68-4500-b946-2fc4f0599e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_dict = {1 : 'male', 0 : 'female'}\n",
    "results = results.replace({'label' : replacement_dict})\n",
    "\n",
    "results.to_csv('predictions.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
